{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YHLee - https://gist.github.com/yhlee-add/00f7aae96e547427fb857e24e9cde1d4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def spherical_grid(size: int, theta: float, phi: float, fov: float = 90.0):\n",
    "    \"\"\"\n",
    "    Compute xyz coordinates of a perspective grid projected on a sphere\n",
    "    \"\"\"\n",
    "    coord_seqs = [np.linspace((size - 1) / size, (1 - size) / size, size) * np.tan(np.radians(fov / 2))] * 2\n",
    "    grid = np.meshgrid(*coord_seqs, indexing='xy')\n",
    "    pos = np.stack([np.ones_like(grid[0]), *grid], axis=-1)\n",
    "\n",
    "    pos /= np.linalg.norm(pos, axis=-1, keepdims=True)\n",
    "    pos = pos.reshape(-1, pos.shape[-1])\n",
    "    pos = R.from_euler('ZY', [-theta, -phi], degrees=True).apply(pos).reshape((size, size, 3))\n",
    "    return pos\n",
    "\n",
    "def erp_grid(pos, erp_h: int = 512):\n",
    "    \"\"\"\n",
    "    Compute position on erp image from xyz coordinates\n",
    "    \"\"\"\n",
    "    lat = np.arcsin(pos[..., 2])\n",
    "    lon = np.arctan2(pos[..., 1], pos[..., 0])\n",
    "    erp_pos = np.stack([0.5 - lat / np.pi, 1 - lon / np.pi], axis=-1) * erp_h\n",
    "    return erp_pos\n",
    "\n",
    "def erp_to_xyz(points, erp_h: int = 512):\n",
    "    \"\"\"\n",
    "    Inverse function of erp_grid\n",
    "    \"\"\"\n",
    "    lat = (0.5 - points[..., 0] / erp_h) * np.pi\n",
    "    lon = (1 - points[..., 1] / erp_h) * np.pi\n",
    "    pos = np.stack([np.cos(lat) * np.cos(lon), np.cos(lat) * np.sin(lon), np.sin(lat)], axis=-1)\n",
    "    return pos\n",
    "\n",
    "def xyz_to_normal_pers(pos, theta: float, phi: float, fov: float = 90.0):\n",
    "    \"\"\"\n",
    "    Funtion to use together with F.grid_sample.\n",
    "    Given a matrix of xyz positions, find the normalized perspective view position.\n",
    "    Also provide mask that indicates valid points.\n",
    "    \"\"\"\n",
    "    h, w, c = pos.shape\n",
    "    assert c == 3\n",
    "    pos = pos.reshape(-1, 3)\n",
    "    pos = R.from_euler('ZY', [-theta, -phi], degrees=True).inv().apply(pos)\n",
    "\n",
    "    mask_x = pos[:, 0] > 1e-8\n",
    "    pos /= -pos[:, :1] # minus sign for matching the axis direction of F.grid_sample\n",
    "    pos /= np.tan(np.radians(fov / 2))\n",
    "\n",
    "    mask_y = (-1 < pos[:, 1]) & (pos[:, 1] < 1)\n",
    "    mask_z = (-1 < pos[:, 2]) & (pos[:, 2] < 1)\n",
    "    mask = mask_x & mask_y & mask_z\n",
    "\n",
    "    return pos[:, 1:].reshape((h, w, 2)), mask.reshape((h, w))\n",
    "\n",
    "\n",
    "directions = [\n",
    "    (0, 0), (45, 0), (22.5, 30), (22.5, -30)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_range = (0, 360)\n",
    "phi_range = (-90, 90)\n",
    "\n",
    "num_phi = 4\n",
    "num_theta = [3, 6, 6, 3]\n",
    "assert num_phi == len(num_theta)\n",
    "\n",
    "directions = []\n",
    "phis = np.linspace(*phi_range, num_phi, endpoint=True)\n",
    "for i in range(num_phi):\n",
    "    thetas = np.linspace(*theta_range, num_theta[i], endpoint=False)\n",
    "    for theta in thetas:\n",
    "        directions.append((theta, phis[i]))\n",
    "    print(*directions[-num_theta[i]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer, logging\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_erp_resolution = 512  # for hiwyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "# stable diffusion RGB output size\n",
    "grid_size = 512\n",
    "\n",
    "class ChoiceSet:\n",
    "    def __init__(self, seq = None, seed = 42):\n",
    "        if seq is None:\n",
    "            self.items = []\n",
    "            self.item_to_pos = {}\n",
    "        else:\n",
    "            self.items = list(seq)\n",
    "            self.item_to_pos = {}\n",
    "            for i, item in enumerate(self.items):\n",
    "                self.item_to_pos[item] = i\n",
    "\n",
    "        self.rng = random.Random(seed)\n",
    "\n",
    "    def add(self, item):\n",
    "        if item in self.item_to_pos:\n",
    "            return\n",
    "        self.item_to_pos[item] = len(self.items)\n",
    "        self.items.append(item)\n",
    "\n",
    "    def remove(self, item):\n",
    "        pos = self.item_to_pos.pop(item)\n",
    "        last_item = self.items.pop()\n",
    "        if pos != len(self.items):\n",
    "            self.items[pos] = last_item\n",
    "            self.item_to_pos[last_item] = pos\n",
    "\n",
    "    def choice(self):\n",
    "        return self.rng.choice(self.items)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return item in self.item_to_pos\n",
    "\n",
    "\n",
    "def generate_points(size: int):\n",
    "    points = np.concatenate([spherical_grid(size, *direction).reshape(-1, 3) for direction in directions])\n",
    "    return points, KDTree(points)\n",
    "\n",
    "def do_matching():\n",
    "    \"\"\"\n",
    "    O(n^2) -> O(n log n) using KDTree\n",
    "    Also made `points` immutable\n",
    "    \"\"\"\n",
    "    points, point_tree = generate_points(grid_size)\n",
    "\n",
    "    # heuristic to make point order not swapped\n",
    "    threshold = (np.pi / (grid_size * 4))\n",
    "\n",
    "    remaining = ChoiceSet(range(len(directions) * grid_size * grid_size))\n",
    "    result_points = []\n",
    "    assignment = {}\n",
    "\n",
    "    with tqdm(total=len(remaining)) as pbar:\n",
    "        while remaining:\n",
    "            t_i = remaining.choice() # target\n",
    "            t_p = points[t_i] # target position. shape: [3]\n",
    "            curr = []\n",
    "\n",
    "            # find all neighbors\n",
    "            neighbors = point_tree.query_ball_point(t_p, threshold)\n",
    "            displacement = points[neighbors] - t_p # shape: [len(neighbors), 3]\n",
    "            dist_square = (displacement * displacement).sum(axis=-1) # shape: [len(neighbors)]\n",
    "\n",
    "            to_merge = [None] * len(directions)\n",
    "\n",
    "            for _, i in sorted(zip(dist_square, neighbors)):\n",
    "                d = i // (grid_size * grid_size) # get which direction grid it is from\n",
    "                if i in remaining and to_merge[d] is None:\n",
    "                    to_merge[d] = i\n",
    "                    remaining.remove(i)\n",
    "                    curr.append(points[i])\n",
    "                    assignment[i] = len(result_points)\n",
    "\n",
    "            result_points.append(curr)\n",
    "            pbar.update(len(curr))\n",
    "\n",
    "    # sanity check: all points must be consumed\n",
    "    assert len(assignment) == len(points)\n",
    "    assert sum(len(l) for l in result_points) == len(points)\n",
    "\n",
    "    return result_points, assignment\n",
    "\n",
    "omni_points, omni_assign = do_matching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "merge_count = [len(ps) for ps in omni_points]\n",
    "\n",
    "cmap = plt.get_cmap('viridis', max(merge_count) - min(merge_count) + 1)\n",
    "\n",
    "sc = ax.scatter(\n",
    "    [ps[0][0] for ps in omni_points],\n",
    "    [ps[0][1] for ps in omni_points],\n",
    "    [ps[0][2] for ps in omni_points],\n",
    "    c=merge_count,\n",
    "    cmap=cmap,\n",
    "    vmin=min(merge_count) - 0.5,\n",
    "    vmax=max(merge_count) + 0.5,\n",
    "    edgecolor=None,\n",
    "    s=0.01,\n",
    ")\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_zlim(-1, 1)\n",
    "ax.set_box_aspect((1, 1, 1))\n",
    "\n",
    "# Add colorbar\n",
    "fig.colorbar(\n",
    "    sc,\n",
    "    ax=ax,\n",
    "    label='Number of merged points',\n",
    "    shrink=0.5,\n",
    "    ticks=np.arange(min(merge_count), max(merge_count) + 1)\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniDiffusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Pretty much copyed code from\n",
    "    https://github.com/omerbt/MultiDiffusion/blob/master/panorama.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        model_key = \"stabilityai/stable-diffusion-2-base\"\n",
    "\n",
    "        self.vae = AutoencoderKL.from_pretrained(model_key, subfolder=\"vae\").to(device)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(model_key, subfolder=\"tokenizer\")\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(model_key, subfolder=\"text_encoder\").to(device)\n",
    "        self.unet = UNet2DConditionModel.from_pretrained(model_key, subfolder=\"unet\").to(device)\n",
    "\n",
    "        self.scheduler = DDIMScheduler.from_pretrained(model_key, subfolder=\"scheduler\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_text_embeds(self, prompt, negative_prompt):\n",
    "        # prompt, negative_prompt: [str]\n",
    "\n",
    "        # Tokenize text and get embeddings\n",
    "        text_input = self.tokenizer(prompt, padding='max_length', max_length=self.tokenizer.model_max_length,\n",
    "                                    truncation=True, return_tensors='pt')\n",
    "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
    "\n",
    "        # Do the same for unconditional embeddings\n",
    "        uncond_input = self.tokenizer(negative_prompt, padding='max_length', max_length=self.tokenizer.model_max_length,\n",
    "                                      return_tensors='pt')\n",
    "\n",
    "        uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
    "\n",
    "        # Cat for final embeddings\n",
    "        text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "        return text_embeddings\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode_latents(self, latents):\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        imgs = self.vae.decode(latents).sample\n",
    "        imgs = (imgs / 2 + 0.5).clamp(0, 1)\n",
    "        return imgs\n",
    "\n",
    "net = OmniDiffusion('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_number_grid(size: int, theta: float, phi: float, fov: float = 90.0):\n",
    "    points_xyz = erp_to_xyz(np.stack(np.meshgrid(np.arange(size) + 0.5, np.arange(size * 2) + 0.5, indexing='ij'), axis=-1), erp_h=size)\n",
    "    points_grid, mask = xyz_to_normal_pers(points_xyz, theta, phi, fov)\n",
    "\n",
    "    # map to 64 by 64 grid\n",
    "    scaled = (points_grid - (-1)) / (2 / 64)\n",
    "    indices = np.clip(np.floor(scaled).astype(int), 0, 63)\n",
    "    indices_x, indices_y = indices[..., 0], indices[..., 1]\n",
    "    linear_indices = indices_y * 64 + indices_x\n",
    "\n",
    "    return np.ma.array(linear_indices, mask=~mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_erp_to_pers(grid, latent_erp, requires_grad=False):\n",
    "    latent_pers = torch.zeros((1, 4, 4096), device=device, requires_grad=requires_grad)\n",
    "    count = torch.zeros(4096, dtype=int, device=device)\n",
    "\n",
    "    index = torch.tensor(grid.compressed(), device=device)\n",
    "\n",
    "    position_i, position_j = np.where(~grid.mask)\n",
    "    source = latent_erp[:, :, position_i, position_j]\n",
    "    ones = torch.ones_like(index)\n",
    "\n",
    "    latent_pers = latent_pers.index_add(dim=2, index=index, source=source)\n",
    "    count = count.index_add(dim=0, index=index, source=ones)\n",
    "\n",
    "    latent_pers = latent_pers / torch.sqrt(count)  # normalize to get unit variance\n",
    "    return latent_pers.reshape((1, 4, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddim_pred_orig(scheduler, model_output, timestep, sample):\n",
    "    # Tweedie's formula\n",
    "    alpha_prod_t = scheduler.alphas_cumprod[timestep]\n",
    "    beta_prod_t = 1 - alpha_prod_t\n",
    "    return (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)\n",
    "\n",
    "def ddim_pred_prev(scheduler, sample, timestep, pred_orig):\n",
    "    # psi of synctweedies paper\n",
    "\n",
    "    alpha_prod_t = scheduler.alphas_cumprod[timestep]\n",
    "    beta_prod_t = 1 - alpha_prod_t\n",
    "\n",
    "    prev_timestep = timestep - scheduler.config.num_train_timesteps // scheduler.num_inference_steps\n",
    "    alpha_prod_t_prev = scheduler.alphas_cumprod[prev_timestep] if prev_timestep >= 0 else scheduler.final_alpha_cumprod\n",
    "\n",
    "    model_output = (sample - alpha_prod_t ** (0.5) * pred_orig) / beta_prod_t ** (0.5)\n",
    "    return alpha_prod_t_prev ** (0.5) * pred_orig + (1 - alpha_prod_t_prev) ** (0.5) * model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_encode(x):\n",
    "    return net.vae.encode(x).latent_dist.sample() * 0.18215\n",
    "\n",
    "def do_decode(z):\n",
    "    return net.vae.decode(z / 0.18215).sample\n",
    "\n",
    "direction_indices = [\n",
    "    [omni_assign[d * grid_size * grid_size + j] for j in range(grid_size * grid_size)]\n",
    "    for d in range(len(directions))\n",
    "]\n",
    "\n",
    "def synchronize_views(orig_pred_list):\n",
    "    value = torch.zeros((1, 3, len(omni_points)), device=device)\n",
    "    count = torch.tensor([len(ps) for ps in omni_points], device=device)\n",
    "\n",
    "    for d, orig_pred in enumerate(orig_pred_list):\n",
    "        x = do_decode(orig_pred)\n",
    "        value[:, :, direction_indices[d]] += x.flatten(2)\n",
    "    value /= count\n",
    "\n",
    "    for d in range(len(directions)):\n",
    "        orig_pred_list[d] = do_encode(value[:, :, direction_indices[d]].reshape((1, 3, 512, 512)))\n",
    "\n",
    "    return orig_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "def prepare_text_embeddings(prompts, negative_prompts=''):\n",
    "    if isinstance(prompts, str):\n",
    "        prompts = [prompts]\n",
    "\n",
    "    if isinstance(negative_prompts, str):\n",
    "        negative_prompts = [negative_prompts]\n",
    "\n",
    "    # Prompts -> text embeds\n",
    "    return net.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def foo():\n",
    "    text_embeds = prepare_text_embeddings(\"A photo of the Dolomites\")\n",
    "    net.scheduler.set_timesteps(50)\n",
    "    guidance_scale = 7.5\n",
    "\n",
    "    grids = [cell_number_grid(latent_erp_resolution, *d) for d in directions]\n",
    "\n",
    "    # step 0\n",
    "    latent_erp = torch.randn((1, 4, latent_erp_resolution, latent_erp_resolution * 2), device=device)\n",
    "\n",
    "    # step 1\n",
    "    latent_pers_list = [\n",
    "        project_erp_to_pers(grid, latent_erp) for grid in grids\n",
    "    ]\n",
    "\n",
    "    for i, t in enumerate(tqdm(net.scheduler.timesteps)):\n",
    "\n",
    "        # step 2\n",
    "        noise_pred_list = []\n",
    "        orig_pred_list = []\n",
    "\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            for d, latent_pers in enumerate(latent_pers_list):\n",
    "                latent_model_input = torch.cat([latent_pers, latent_pers])\n",
    "                noise_pred = net.unet(latent_model_input, t, encoder_hidden_states=text_embeds)[\"sample\"]\n",
    "\n",
    "                noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "                noise_pred_list.append(noise_pred)\n",
    "\n",
    "                orig_pred = ddim_pred_orig(net.scheduler, noise_pred, t, latent_pers)\n",
    "                orig_pred_list.append(orig_pred)\n",
    "\n",
    "        # sync tweedies\n",
    "        orig_pred_list = synchronize_views(orig_pred_list)\n",
    "\n",
    "        # denoise each pers view\n",
    "        latent_pers_list = [\n",
    "            ddim_pred_prev(net.scheduler, sample, t, orig_pred)\n",
    "            for orig_pred, sample in zip(orig_pred_list, latent_pers_list)\n",
    "        ]\n",
    "\n",
    "    # decode\n",
    "    result = [\n",
    "        T.ToPILImage()(net.decode_latents(latent_pers)[0])\n",
    "        for latent_pers in latent_pers_list\n",
    "    ]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "res = foo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection for final result\n",
    "\n",
    "rgb_erp_resolution = 768\n",
    "\n",
    "def project_direction(img, direction):\n",
    "    flow, mask = xyz_to_normal_pers(erp_to_xyz(\n",
    "        np.stack(np.meshgrid(np.arange(rgb_erp_resolution) + 0.5, np.arange(rgb_erp_resolution * 2) + 0.5, indexing='ij'), axis=-1),\n",
    "        erp_h=rgb_erp_resolution\n",
    "    ), *direction)\n",
    "\n",
    "    flow = torch.tensor(flow[np.newaxis], device='cuda', dtype=torch.float32)\n",
    "    mask = torch.tensor(mask, device='cuda')\n",
    "    sampled = nn.functional.grid_sample(img, flow, mode='bilinear', padding_mode='border', align_corners=False)\n",
    "    return sampled[0] * mask, mask\n",
    "\n",
    "def project_direction_all(imgs):\n",
    "    count = torch.zeros((rgb_erp_resolution, rgb_erp_resolution * 2), dtype=torch.int32, device='cuda')\n",
    "    value = torch.zeros((1, 3, rgb_erp_resolution, rgb_erp_resolution * 2), dtype=torch.float32, device='cuda')\n",
    "\n",
    "    for img, direction in zip(imgs, directions):\n",
    "        result = project_direction(img, direction)\n",
    "        value += result[0]\n",
    "        count += result[1]\n",
    "\n",
    "    count[count == 0] = 1\n",
    "    return value / count\n",
    "\n",
    "T.ToPILImage()(\n",
    "    project_direction_all([T.ToTensor()(img)[None].to(device) for img in res])[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: with matplotlib, subplot images. use `res` which is a list of PIL images with length 4\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 4.5))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(res[i])\n",
    "    ax.set_title(str(directions[i]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_phi):\n",
    "    fig, axes = plt.subplots(1, num_theta[i], figsize=(5 * num_theta[i], 5))\n",
    "    for j, ax in enumerate(axes.flat):\n",
    "        ax.imshow(res[i * num_theta[i] + j])\n",
    "        ax.set_title(str(directions[i * num_theta[i] + j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotating Directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "theta_range = (0, 360)\n",
    "phi_range = (-45, 45)\n",
    "\n",
    "num_phi = 4\n",
    "num_theta = [3, 4, 4, 3]\n",
    "assert num_phi == len(num_theta)\n",
    "\n",
    "directions = []\n",
    "phis = np.linspace(*phi_range, num_phi, endpoint=True)\n",
    "for i in range(num_phi):\n",
    "    thetas = np.linspace(*theta_range, num_theta[i], endpoint=False)\n",
    "    for theta in thetas:\n",
    "        directions.append((theta, phis[i]))\n",
    "    print(*directions[-num_theta[i]:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_matching(directions):\n",
    "    \"\"\"\n",
    "    O(n^2) -> O(n log n) using KDTree\n",
    "    Also made `points` immutable\n",
    "    \"\"\"\n",
    "    points, point_tree = generate_points(grid_size)\n",
    "\n",
    "    # heuristic to make point order not swapped\n",
    "    threshold = (np.pi / (grid_size * 4))\n",
    "\n",
    "    remaining = ChoiceSet(range(len(directions) * grid_size * grid_size))\n",
    "    result_points = []\n",
    "    assignment = {}\n",
    "\n",
    "    with tqdm(total=len(remaining)) as pbar:\n",
    "        while remaining:\n",
    "            t_i = remaining.choice() # target\n",
    "            t_p = points[t_i] # target position. shape: [3]\n",
    "            curr = []\n",
    "\n",
    "            # find all neighbors\n",
    "            neighbors = point_tree.query_ball_point(t_p, threshold)\n",
    "            displacement = points[neighbors] - t_p # shape: [len(neighbors), 3]\n",
    "            dist_square = (displacement * displacement).sum(axis=-1) # shape: [len(neighbors)]\n",
    "\n",
    "            to_merge = [None] * len(directions)\n",
    "\n",
    "            for _, i in sorted(zip(dist_square, neighbors)):\n",
    "                d = i // (grid_size * grid_size) # get which direction grid it is from\n",
    "                if i in remaining and to_merge[d] is None:\n",
    "                    to_merge[d] = i\n",
    "                    remaining.remove(i)\n",
    "                    curr.append(points[i])\n",
    "                    assignment[i] = len(result_points)\n",
    "\n",
    "            result_points.append(curr)\n",
    "            pbar.update(len(curr))\n",
    "\n",
    "    # sanity check: all points must be consumed\n",
    "    assert len(assignment) == len(points)\n",
    "    assert sum(len(l) for l in result_points) == len(points)\n",
    "\n",
    "    return result_points, assignment\n",
    "\n",
    "def get_rotated_directions(base_directions, timestep, total_timesteps, rotation_per_step):\n",
    "    \"\"\"\n",
    "    Apply horizontal rotation to directions based on the current timestep.\n",
    "    :param base_directions: List of original (theta, phi) directions.\n",
    "    :param timestep: Current diffusion timestep.\n",
    "    :param total_timesteps: Total number of diffusion timesteps.\n",
    "    :param rotation_per_step: Horizontal rotation angle per timestep in degrees.\n",
    "    :return: List of modified directions with horizontal rotation applied.\n",
    "    \"\"\"\n",
    "    rotation_offset = rotation_per_step * (timestep / total_timesteps)  # Calculate rotation offset\n",
    "    modified_directions = [\n",
    "        (theta, (phi + rotation_offset) % 360)  # Add horizontal rotation to phi\n",
    "        for theta, phi in base_directions\n",
    "    ]\n",
    "    return modified_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def foo_with_rotation_and_rematching(rotation_per_step=10.0):\n",
    "    \"\"\"\n",
    "    Modified function with horizontal rotation and dynamic rematching per timestep.\n",
    "    :param rotation_per_step: Degrees of horizontal rotation per timestep.\n",
    "    \"\"\"\n",
    "    text_embeds = prepare_text_embeddings(\"A photo of the Dolomites\")\n",
    "    net.scheduler.set_timesteps(50)\n",
    "    guidance_scale = 7.5\n",
    "\n",
    "    # Initialize ERP latent\n",
    "    latent_erp = torch.randn((1, 4, latent_erp_resolution, latent_erp_resolution * 2), device=device)\n",
    "\n",
    "    # Iterate through timesteps\n",
    "    for i, t in enumerate(tqdm(net.scheduler.timesteps)):\n",
    "        noise_pred_list = []\n",
    "        orig_pred_list = []\n",
    "\n",
    "        # Calculate rotated directions for the current timestep\n",
    "        rotated_directions = get_rotated_directions(\n",
    "            directions, timestep=i, total_timesteps=len(net.scheduler.timesteps), rotation_per_step=rotation_per_step\n",
    "        )\n",
    "\n",
    "        # Perform do_matching for the rotated directions\n",
    "        omni_points, omni_assign = do_matching(rotated_directions)\n",
    "\n",
    "        # Update direction_indices based on the new `omni_assign`\n",
    "        direction_indices = [\n",
    "            [omni_assign[d * grid_size * grid_size + j] for j in range(grid_size * grid_size)]\n",
    "            for d in range(len(rotated_directions))\n",
    "        ]\n",
    "\n",
    "        # Update grids for the current timestep\n",
    "        grids = [cell_number_grid(latent_erp_resolution, *d) for d in rotated_directions]\n",
    "\n",
    "        # Update perspective views\n",
    "        latent_pers_list = [\n",
    "            project_erp_to_pers(grid, latent_erp) for grid in grids\n",
    "        ]\n",
    "\n",
    "        # Denoising step\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            for d, latent_pers in enumerate(latent_pers_list):\n",
    "                latent_model_input = torch.cat([latent_pers, latent_pers])\n",
    "                noise_pred = net.unet(latent_model_input, t, encoder_hidden_states=text_embeds)[\"sample\"]\n",
    "\n",
    "                noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "                noise_pred_list.append(noise_pred)\n",
    "\n",
    "                orig_pred = ddim_pred_orig(net.scheduler, noise_pred, t, latent_pers)\n",
    "                orig_pred_list.append(orig_pred)\n",
    "\n",
    "        # Synchronize views across directions\n",
    "        orig_pred_list = synchronize_views(orig_pred_list)\n",
    "\n",
    "        # Update latent_pers_list for the next timestep\n",
    "        latent_pers_list = [\n",
    "            ddim_pred_prev(net.scheduler, sample, t, orig_pred)\n",
    "            for orig_pred, sample in zip(orig_pred_list, latent_pers_list)\n",
    "        ]\n",
    "\n",
    "    # Decode final images\n",
    "    result = [\n",
    "        T.ToPILImage()(net.decode_latents(latent_pers)[0])\n",
    "        for latent_pers in latent_pers_list\n",
    "    ]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Run the modified pipeline\n",
    "res = foo_with_rotation_and_rematching(rotation_per_step=10)  # 10 degrees of horizontal rotation per timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def precompute_matching(rotation_per_step, total_timesteps):\n",
    "    \"\"\"\n",
    "    Precompute matching results for all timesteps with rotated directions.\n",
    "    :param rotation_per_step: Degrees of horizontal rotation per timestep.\n",
    "    :param total_timesteps: Total number of timesteps.\n",
    "    :return: Precomputed matching results for all timesteps.\n",
    "    \"\"\"\n",
    "    precomputed_matches = []\n",
    "\n",
    "    for timestep in range(total_timesteps):\n",
    "        # Calculate rotated directions\n",
    "        rotated_directions = get_rotated_directions(\n",
    "            directions, timestep=timestep, total_timesteps=total_timesteps, rotation_per_step=rotation_per_step\n",
    "        )\n",
    "\n",
    "        # Perform matching for rotated directions\n",
    "        omni_points, omni_assign = do_matching(rotated_directions)\n",
    "\n",
    "        # Save results\n",
    "        precomputed_matches.append((rotated_directions, omni_points, omni_assign))\n",
    "\n",
    "    return precomputed_matches\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def foo_with_precomputed_matching(precomputed_matches):\n",
    "    \"\"\"\n",
    "    Modified function using precomputed matching results.\n",
    "    :param precomputed_matches: Precomputed matching results for all timesteps.\n",
    "    \"\"\"\n",
    "    text_embeds = prepare_text_embeddings(\"A photo of the Dolomites\")\n",
    "    net.scheduler.set_timesteps(50)\n",
    "    guidance_scale = 7.5\n",
    "\n",
    "    # Initialize ERP latent\n",
    "    latent_erp = torch.randn((1, 4, latent_erp_resolution, latent_erp_resolution * 2), device=device)\n",
    "\n",
    "    # Iterate through timesteps\n",
    "    for i, t in enumerate(tqdm(net.scheduler.timesteps)):\n",
    "        noise_pred_list = []\n",
    "        orig_pred_list = []\n",
    "\n",
    "        # Load precomputed matching results for the current timestep\n",
    "        rotated_directions, omni_points, omni_assign = precomputed_matches[i]\n",
    "\n",
    "        # Update direction_indices based on the precomputed `omni_assign`\n",
    "        direction_indices = [\n",
    "            [omni_assign[d * grid_size * grid_size + j] for j in range(grid_size * grid_size)]\n",
    "            for d in range(len(rotated_directions))\n",
    "        ]\n",
    "\n",
    "        # Update grids for the current timestep\n",
    "        grids = [cell_number_grid(latent_erp_resolution, *d) for d in rotated_directions]\n",
    "\n",
    "        # Update perspective views\n",
    "        latent_pers_list = [\n",
    "            project_erp_to_pers(grid, latent_erp) for grid in grids\n",
    "        ]\n",
    "\n",
    "        # Denoising step\n",
    "        with torch.autocast(\"cuda\"):\n",
    "            for d, latent_pers in enumerate(latent_pers_list):\n",
    "                latent_model_input = torch.cat([latent_pers, latent_pers])\n",
    "                noise_pred = net.unet(latent_model_input, t, encoder_hidden_states=text_embeds)[\"sample\"]\n",
    "\n",
    "                noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "                noise_pred_list.append(noise_pred)\n",
    "\n",
    "                orig_pred = ddim_pred_orig(net.scheduler, noise_pred, t, latent_pers)\n",
    "                orig_pred_list.append(orig_pred)\n",
    "\n",
    "        # Synchronize views across directions\n",
    "        orig_pred_list = synchronize_views(orig_pred_list)\n",
    "\n",
    "        # Update latent_pers_list for the next timestep\n",
    "        latent_pers_list = [\n",
    "            ddim_pred_prev(net.scheduler, sample, t, orig_pred)\n",
    "            for orig_pred, sample in zip(orig_pred_list, latent_pers_list)\n",
    "        ]\n",
    "\n",
    "    # Decode final images\n",
    "    result = [\n",
    "        T.ToPILImage()(net.decode_latents(latent_pers)[0])\n",
    "        for latent_pers in latent_pers_list\n",
    "    ]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Precompute matching results\n",
    "total_timesteps = 50\n",
    "rotation_per_step = 360 // total_timesteps\n",
    "precomputed_matches = precompute_matching(rotation_per_step, total_timesteps)\n",
    "\n",
    "# Run the modified pipeline\n",
    "res = foo_with_precomputed_matching(precomputed_matches)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
